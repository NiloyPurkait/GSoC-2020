{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RDF_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiloyPurkait/GSoC-2020/blob/master/RDF_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D_hwa5V6g_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znwqgofyaAMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI2mSRNbiF3O",
        "colab_type": "code",
        "outputId": "abee37a9-d5b6-476b-e406-30a51a6fdd2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amM8fHIvUp3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#tiny number\n",
        "_NEG_INF = -1e8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ASQmtAEJHsM",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Helper functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5IA2GOAUs5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def _tensorize(vocab, text):\n",
        "    \"\"\"\n",
        "    Function to convert texts into number sequences first, and then\n",
        "    add padding. Basically, tensorising them.\n",
        "    :param vocab: The vocab which is used to lookup ids\n",
        "    :type vocab: tf.tokenizer obj\n",
        "    :param text: A list of sentences or a text file\n",
        "    :type text: list\n",
        "    :return: tensorised text data\n",
        "    :rtype: tf.tensor\n",
        "    \"\"\"\n",
        "    tensor = vocab.texts_to_sequences(text)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                           padding='post')\n",
        "\n",
        "    return tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxjX4XQQbbLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lquXulVcbbI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def padding(tensor, max_length):\n",
        "    \"\"\"\n",
        "    Pads the given tensor to a maximum sequence length along\n",
        "    axis 1.\n",
        "    for ex -\n",
        "    let the tensor be [1,2,3,4] if th given max_length is 5\n",
        "    the tensor becomes [1,2,34,0]\n",
        "    Mostly used to pad the target sentences of the multilingual\n",
        "    model and the node_list of all models,\n",
        "\n",
        "    :param tensor:A tf tensor\n",
        "    :type tensor:tf.tensor\n",
        "    :param max_length:Dimension along axis 1, of the new tensor\n",
        "    :type max_length:int\n",
        "    :return:The padded tensor\n",
        "    :rtype:tf tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    padding = tf.constant([[0, 0], [0, max_length - tensor.shape[1]]])\n",
        "    padded_tensor = tf.pad(tensor, padding, mode='CONSTANT')\n",
        "\n",
        "    return padded_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttukSCrtbzFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from src.DataLoader imports"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aOeoCQwJRGS",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSrGgzoUbZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LoadGatDataset(train_path, eval_path, test_path, srv_vocab,\n",
        "                   tgt_vocab, lang, num_examples=None):\n",
        "    train_ = {}\n",
        "    eval_ = {}\n",
        "    test_ = {}\n",
        "\n",
        "\n",
        "    # load the train and eval datasets\n",
        "    with open(train_path, 'rb') as f:\n",
        "        train_set = pickle.load(f)\n",
        "    with open(eval_path, 'rb') as f:\n",
        "        eval_set = pickle.load(f)\n",
        "    with open(test_path, 'rb') as f:\n",
        "        test_set = pickle.load(f)\n",
        "    with open(srv_vocab, 'rb') as f:\n",
        "        src_vocab = pickle.load(f)\n",
        "\n",
        "    train_input, train_tgt = zip(*train_set)\n",
        "    eval_input, eval_tgt = zip(*eval_set)\n",
        "    (train_nodes, train_labels, train_node1, train_node2) = zip(*train_input)\n",
        "    (eval_nodes, eval_labels, eval_node1, eval_node2) = zip(*eval_input)\n",
        "    (test_nodes, test_labels, test_node1, test_node2) = zip(*test_set)\n",
        "\n",
        "    train_[\"train_node_tensor\"] = _tensorize(src_vocab, train_nodes)\n",
        "    train_[\"train_label_tensor\"] = _tensorize(src_vocab, train_labels)\n",
        "    train_[\"train_node1_tensor\"] = _tensorize(src_vocab, train_node1)\n",
        "    train_[\"train_node2_tensor\"] = _tensorize(src_vocab, train_node2)\n",
        "\n",
        "    eval_[\"eval_node_tensor\"] = _tensorize(src_vocab, eval_nodes)\n",
        "    eval_[\"eval_label_tensor\"] = _tensorize(src_vocab, eval_labels)\n",
        "    eval_[\"eval_node1_tensor\"] = _tensorize(src_vocab, eval_node1)\n",
        "    eval_[\"eval_node2_tensor\"] = _tensorize(src_vocab, eval_node2)\n",
        "\n",
        "    test_[\"test_node_tensor\"] = _tensorize(src_vocab, test_nodes)\n",
        "    test_[\"test_label_tensor\"] = _tensorize(src_vocab, test_labels)\n",
        "    test_[\"test_node1_tensor\"] = _tensorize(src_vocab, test_node1)\n",
        "    test_[\"test_node2_tensor\"] = _tensorize(src_vocab, test_node1)\n",
        "\n",
        "\n",
        "    train_tgt_tensor = src_vocab.texts_to_sequences(train_tgt)\n",
        "    train_[\"train_tgt_tensor\"] = tf.keras.preprocessing.sequence.pad_sequences(train_tgt_tensor, padding='post')\n",
        "    eval_tgt_tensor = src_vocab.texts_to_sequences(eval_tgt)\n",
        "    eval_[\"eval_tgt_tensor\"] = tf.keras.preprocessing.sequence.pad_sequences(eval_tgt_tensor, padding='post')\n",
        "    target_vocab = src_vocab\n",
        "\n",
        "    return (train_, eval_, test_, src_vocab, target_vocab, max_length(train_tgt_tensor))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vcoOnT3UPor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetGATDataset(train_path, eval_path,\n",
        "                  test_path, src_vocab,\n",
        "                  tgt_vocab, lang,\n",
        "                  set=None):\n",
        "  \n",
        "\n",
        "    (train, eval, test, src_vocab, tgt_vocab, max_length_targ) = LoadGatDataset(train_path,\n",
        "                                                                                eval_path,\n",
        "                                                                                test_path, src_vocab,\n",
        "                                                                                tgt_vocab, lang)\n",
        "    node_tensor = padding(train[\"train_node_tensor\"], 16)\n",
        "    label_tensor = padding(train[\"train_label_tensor\"], 16)\n",
        "    node1_tensor = padding(train[\"train_node1_tensor\"], 16)\n",
        "    node2_tensor = padding(train[\"train_node2_tensor\"], 16)\n",
        "\n",
        "    eval_nodes = padding(eval[\"eval_node_tensor\"], 16)\n",
        "    eval_labels = padding(eval[\"eval_label_tensor\"], 16)\n",
        "    eval_node1 = padding(eval[\"eval_node1_tensor\"], 16)\n",
        "    eval_node2 = padding(eval[\"eval_node2_tensor\"], 16)\n",
        "\n",
        "    test_nodes = padding(test[\"test_node_tensor\"], 16)\n",
        "    test_labels = padding(test[\"test_label_tensor\"], 16)\n",
        "    test_node1 = padding(test[\"test_node1_tensor\"], 16)\n",
        "    test_node2 = padding(test[\"test_node2_tensor\"], 16)\n",
        "\n",
        "    print('\\nTrain Tensor shapes (nodes, labels, node1, node2, target) : ')\n",
        "    print(node_tensor.shape, label_tensor.shape, node1_tensor.shape, node2_tensor.shape, train[\"train_tgt_tensor\"].shape)\n",
        "    print('\\nEval Tensor shapes (nodes, labes, node1, node2) : ')\n",
        "    print(eval_nodes.shape, eval_labels.shape, eval_node1.shape, eval_node2.shape, eval[\"eval_tgt_tensor\"].shape)\n",
        "    print('\\nTest Tensor shapes (nodes, labes, node1, node2) : ')\n",
        "    print(test_nodes.shape, test_labels.shape, test_node1.shape, test_node2.shape)\n",
        "\n",
        "    TRAIN_BUFFER_SIZE = len(train[\"train_tgt_tensor\"])\n",
        "    EVAL_BUFFER_SIZE = len(eval[\"eval_tgt_tensor\"])\n",
        "    BATCH_SIZE = batch_size\n",
        "    steps_per_epoch = len(train[\"train_tgt_tensor\"]) // BATCH_SIZE\n",
        "    src_vocab_size = len(src_vocab.word_index) + 1\n",
        "\n",
        "    tgt_vocab_size = len(tgt_vocab.word_index) + 1\n",
        "\n",
        "    dataset_size = train[\"train_tgt_tensor\"].shape[0]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((node_tensor, label_tensor,\n",
        "                                                  node1_tensor, node2_tensor, train[\"train_tgt_tensor\"])).shuffle(TRAIN_BUFFER_SIZE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    eval_set = tf.data.Dataset.from_tensor_slices((eval_nodes, eval_labels,\n",
        "                                                   eval_node1, eval_node2, eval[\"eval_tgt_tensor\"])).shuffle(EVAL_BUFFER_SIZE)\n",
        "    eval_set = eval_set.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    test_set = tf.data.Dataset.from_tensor_slices((test_nodes, test_labels,\n",
        "                                                   test_node1, test_node2))\n",
        "    test_set = test_set.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "    if set == None:\n",
        "        return (dataset, eval_set, test_set, TRAIN_BUFFER_SIZE, BATCH_SIZE, steps_per_epoch,\n",
        "                src_vocab_size, src_vocab, tgt_vocab_size, tgt_vocab,\n",
        "                max_length_targ, dataset_size)\n",
        "    elif set == 'test':\n",
        "        return (test_set, TRAIN_BUFFER_SIZE, BATCH_SIZE, steps_per_epoch,\n",
        "                src_vocab_size, src_vocab, tgt_vocab_size, tgt_vocab)\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Z6uW98JZHx",
        "colab_type": "text"
      },
      "source": [
        "## Load processed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27UyhuV0gJYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "train_path = '/content/gdrive/My Drive/data/reif_train'\n",
        "eval_path = '/content/gdrive/My Drive/data/reif_eval'\n",
        "test_path = '/content/gdrive/My Drive/data/reif_test'\n",
        "src_vocab = '/content/gdrive/My Drive/data/reif_src_vocab'\n",
        "tgt_vocab =  '/content/gdrive/My Drive/data/train_vocab.model'\n",
        "\n",
        "\n",
        "lang = 'eng'\n",
        "batch_size = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k68L7MMxdqq0",
        "colab_type": "code",
        "outputId": "63de32ab-a2a5-4843-91c2-f537acfc99cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "(dataset, eval_set, test_set, BUFFER_SIZE, BATCH_SIZE, steps_per_epoch,\n",
        "     src_vocab_size, src_vocab, tgt_vocab_size, tgt_vocab, max_length_targ, dataset_size) = GetGATDataset(train_path, eval_path,\n",
        "                                                                                                     test_path, src_vocab,\n",
        "                                                                                                      tgt_vocab, lang)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Tensor shapes (nodes, labels, node1, node2, target) : \n",
            "(34352, 16) (34352, 16) (34352, 16) (34352, 16) (34352, 82)\n",
            "\n",
            "Eval Tensor shapes (nodes, labes, node1, node2) : \n",
            "(4316, 16) (4316, 16) (4316, 16) (4316, 16) (4316, 70)\n",
            "\n",
            "Test Tensor shapes (nodes, labes, node1, node2) : \n",
            "(4224, 16) (4224, 16) (4224, 16) (4224, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGNqXHDqJnCS",
        "colab_type": "text"
      },
      "source": [
        "## Probe the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abUI0uFMlYlY",
        "colab_type": "code",
        "outputId": "e7397691-13f4-4767-b1e9-15aa2db8e456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tgt_vocab_size, src_vocab_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10248, 10248)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfPun4cgoGxT",
        "colab_type": "code",
        "outputId": "a9f74b83-cd4f-488e-831e-607d7725a387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " dataset_size, BUFFER_SIZE, BATCH_SIZE, steps_per_epoch"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34352, 34352, 2, 17176)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDYoDAVsJskI",
        "colab_type": "text"
      },
      "source": [
        "## Make example input and target batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HT4aIZEtmZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Example inputs and target batch for testing\n",
        "\n",
        "nodes_, labels_, node1_, node2_, target_ = next(iter(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Qew4p4uaBx",
        "colab_type": "code",
        "outputId": "59de428e-5069-4956-e04f-ef573eff895c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        " def convert_to_string(target_, n):\n",
        "    '''\n",
        "    args:\n",
        "      target_ : target tensor batch\n",
        "      n = nth element in batch \n",
        "\n",
        "    usage:\n",
        "      Takes in a tensor of (batch_size, vocab_size) and\n",
        "      converts a batch instance into its string equivalent\n",
        "    '''\n",
        "    sequence = [tf.reduce_sum(i).numpy() for i in target_[n]]\n",
        "    text = ' '.join([src_vocab.index_word[i] if i!=0 else '<pad>' for i in sequence])\n",
        "    print(text)\n",
        "\n",
        "convert_to_string(target_, 1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> banyumasan people , one of the ethnic groups found in java , in the region of malaysia eat ayam penyet . <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBWSiLCQJ_21",
        "colab_type": "text"
      },
      "source": [
        "# Implement Encoder Module Layers:\n",
        "To assemble the encoder modules, we must first define the following layers:\n",
        "- Shared Embedding Layer\n",
        "- Graph Attention Layer\n",
        "- Feed Forward Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-1kqlQDJzn_",
        "colab_type": "text"
      },
      "source": [
        "### Define Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UhyafJAueN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embedding layer\n",
        "\n",
        "class EmbeddingSharedWeights(tf.keras.layers.Layer):\n",
        "    \"\"\"Calculates input embeddings and pre-softmax linear with shared weights.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        \"\"\"Specify characteristic parameters of embedding layer.\n",
        "\n",
        "        Args:\n",
        "          vocab_size: Number of tokens in the embedding. (Typically ~32,000)\n",
        "          hidden_size: Dimensionality of the embedding. (Typically 512 or 1024)\n",
        "        \"\"\"\n",
        "        super(EmbeddingSharedWeights, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build embedding layer.\"\"\"\n",
        "        with tf.name_scope(\"embedding_and_softmax\"):\n",
        "            # Create and initialize weights. The random normal initializer was chosen\n",
        "            # arbitrarily, and works well.\n",
        "            self.shared_weights = self.add_weight(\n",
        "                \"weights\",\n",
        "                shape=[self.vocab_size, self.hidden_size],\n",
        "                dtype=\"float32\",\n",
        "                initializer=tf.random_normal_initializer(\n",
        "                    mean=0., stddev=self.hidden_size ** -0.5))\n",
        "        super(EmbeddingSharedWeights, self).build(input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "        }\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\"):\n",
        "        \"\"\"Get token embeddings of inputs.\n",
        "\n",
        "        Args:\n",
        "          inputs: An int64 tensor with shape [batch_size, length]\n",
        "          mode: string, a valid value is one of \"embedding\" and \"linear\".\n",
        "        Returns:\n",
        "          outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\n",
        "            shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\n",
        "            linear tensor, float32 with shape [batch_size, length, vocab_size].\n",
        "        Raises:\n",
        "          ValueError: if mode is not valid.\n",
        "        \"\"\"\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs)\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        else:\n",
        "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
        "\n",
        "    def _embedding(self, inputs):\n",
        "        \"\"\"Applies embedding based on inputs tensor.\"\"\"\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            # Create binary mask of size [batch_size, length]\n",
        "            mask = tf.cast(tf.not_equal(inputs, 0), tf.float32)\n",
        "            embeddings = tf.gather(self.shared_weights, inputs)\n",
        "            embeddings *= tf.expand_dims(mask, -1)\n",
        "            # Scale embedding by the sqrt of the hidden size\n",
        "            embeddings *= self.hidden_size ** 0.5\n",
        "\n",
        "            return embeddings\n",
        "\n",
        "    def _linear(self, inputs):\n",
        "        \"\"\"Computes logits by running inputs through a linear layer.\n",
        "\n",
        "        Args:\n",
        "          inputs: A float32 tensor with shape [batch_size, length, hidden_size]\n",
        "        Returns:\n",
        "          float32 tensor with shape [batch_size, length, vocab_size].\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"presoftmax_linear\"):\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            length = tf.shape(inputs)[1]\n",
        "\n",
        "            x = tf.reshape(inputs, [-1, self.hidden_size])\n",
        "            logits = tf.matmul(x, self.shared_weights, transpose_b=True)\n",
        "\n",
        "            return tf.reshape(logits, [batch_size, length, self.vocab_size])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zUDL_c7J4hq",
        "colab_type": "text"
      },
      "source": [
        "Define Graph Attention Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5owA1sCzLdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder layers = Embedding shared weights + GA Layer + FFN Layer\n",
        "\n",
        "\n",
        "class GraphAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, num_heads, reg_scale=0.001, rate=0.1):\n",
        "        \"\"\"\n",
        "        Graph Attention Network Layer, takes input and returns embedded\n",
        "        node features with self attention applied on the feature matrix\n",
        "        \"\"\"\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.in_dim = d_model\n",
        "        self.out_dim = dff\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = rate\n",
        "        self.kernels = []\n",
        "        self.biases = []\n",
        "        self.attn_kernels = []\n",
        "\n",
        "        self.lrelu = tf.keras.layers.LeakyReLU()\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.reg = tf.keras.regularizers.l2(l=reg_scale)\n",
        "\n",
        "        for head in range(self.num_heads):\n",
        "            kernel = self.add_weight(shape=(self.in_dim, self.out_dim),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     regularizer=self.reg,\n",
        "                                     name='kernel_{}'.format(head))\n",
        "            bias = self.add_weight(shape=(self.out_dim,),\n",
        "                                   initializer='glorot_uniform',\n",
        "                                   regularizer=self.reg,\n",
        "                                   name='bias_{}'.format(head))\n",
        "            self.kernels.append([kernel, bias])\n",
        "            # Attention kernels\n",
        "            attn_kernel_self = self.add_weight(shape=(self.out_dim, 1),\n",
        "                                               initializer='glorot_uniform',\n",
        "                                               regularizer=self.reg,\n",
        "                                               name='attn_kernel_self_{}'.format(head))\n",
        "            attn_kernel_neighs = self.add_weight(shape=(self.out_dim, 1),\n",
        "                                                 initializer='glorot_uniform',\n",
        "                                                 regularizer=self.reg,\n",
        "                                                 name='attn_kernel_neigh_{}'.format(head))\n",
        "            self.attn_kernels.append([attn_kernel_self, attn_kernel_neighs])\n",
        "\n",
        "    def call(self, nodes):\n",
        "        inputs = nodes\n",
        "\n",
        "        outputs = []\n",
        "        for head in range(self.num_heads):\n",
        "            kernel = self.kernels[head]\n",
        "            attention_kernel = self.attn_kernels[head]\n",
        "            features = tf.keras.backend.dot(inputs, kernel[0])\n",
        "            features = tf.add(features, kernel[1])\n",
        "            attn_for_self = tf.keras.backend.dot(features, attention_kernel[0])\n",
        "            attn_for_neighs = tf.keras.backend.dot(features, attention_kernel[1])\n",
        "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
        "            dense = tf.matmul(attn_for_self, attn_for_neighs, transpose_b=True)\n",
        "            dense = self.lrelu(dense)\n",
        "\n",
        "            # Mask values before activation (Vaswani et al., 2017)\n",
        "            # mask_local = -10e9 * (1.0 - adj)\n",
        "            # dense += mask_local\n",
        "\n",
        "            # Apply softmax to get attention coefficients\n",
        "            dense = tf.math.softmax(dense)  # (N x N)\n",
        "\n",
        "            # Apply dropout to features and attention coefficients\n",
        "            if self.trainable:\n",
        "                dense = self.dropout(dense)  # (N x N)\n",
        "                features = self.dropout(features)  # (N x F')\n",
        "\n",
        "            # Linear combination with neighbors' features\n",
        "            node_features = tf.matmul(dense, features)  # (N x F')\n",
        "            outputs.append(node_features)\n",
        "\n",
        "        output = tf.reduce_mean(tf.stack(outputs), axis=0)  # N x F')\n",
        "        output = tf.nn.relu(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ejnNmdKM-v",
        "colab_type": "text"
      },
      "source": [
        "### Define Feed-Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc139I1TzOFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feed forward layer\n",
        "\n",
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    \"\"\"Fully connected feedforward network.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, filter_size, relu_dropout):\n",
        "        \"\"\"Initialize FeedForwardNetwork.\n",
        "\n",
        "        Args:\n",
        "          hidden_size: int, output dim of hidden layer.\n",
        "          filter_size: int, filter size for the inner (first) dense layer.\n",
        "          relu_dropout: float, dropout rate for training.\n",
        "        \"\"\"\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.filter_size = filter_size\n",
        "        self.relu_dropout = relu_dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.filter_dense_layer = tf.keras.layers.Dense(\n",
        "            self.filter_size,\n",
        "            use_bias=True,\n",
        "            activation=tf.nn.relu,\n",
        "            name=\"filter_layer\")\n",
        "        self.output_dense_layer = tf.keras.layers.Dense(\n",
        "            self.hidden_size, use_bias=True, name=\"output_layer\")\n",
        "        super(FeedForwardNetwork, self).build(input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"filter_size\": self.filter_size,\n",
        "            \"relu_dropout\": self.relu_dropout,\n",
        "        }\n",
        "\n",
        "    def call(self, x, training):\n",
        "        \"\"\"Return outputs of the feedforward network.\n",
        "\n",
        "        Args:\n",
        "          x: tensor with shape [batch_size, length, hidden_size]\n",
        "          training: boolean, whether in training mode or not.\n",
        "\n",
        "        Returns:\n",
        "          Output of the feedforward network.\n",
        "          tensor with shape [batch_size, length, hidden_size]\n",
        "        \"\"\"\n",
        "        # Retrieve dynamically known shapes\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        length = tf.shape(x)[1]\n",
        "\n",
        "        output = self.filter_dense_layer(x)\n",
        "        if training:\n",
        "            output = tf.nn.dropout(output, rate=self.relu_dropout)\n",
        "        output = self.output_dense_layer(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAZBfEI_KQdI",
        "colab_type": "text"
      },
      "source": [
        "## Assemble Graph Encoder module\n",
        "We put together the encoder module using the defined layers above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlBJi_Ytzdjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Graph Encoder Layer\n",
        "\n",
        "class GraphEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, trainable, dff,\n",
        "                 filter_size, reg_scale=0.001, rate=0.1):\n",
        "\n",
        "        super(GraphEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.node_role_layer = tf.keras.layers.Dense(self.d_model, input_shape=(2 * d_model,))\n",
        "        self.enc_layers = []\n",
        "        for _ in range(num_layers):\n",
        "            gat_layer = GraphAttentionLayer(d_model, dff, num_heads,\n",
        "                                            reg_scale=reg_scale, rate=rate)\n",
        "            ffn_layer = FeedForwardNetwork(dff, filter_size, rate)\n",
        "            self.enc_layers.append([gat_layer, ffn_layer])\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.edge_layer = tf.keras.layers.Dense(self.d_model)\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def call(self, node_tensor, label_tensor, node1_tensor, node2_tensor):\n",
        "        # adding embedding and position encoding.\n",
        "\n",
        "        edge_tensor = tf.concat([node1_tensor, node2_tensor], 2)\n",
        "        edge_tensor = tf.cast(self.node_role_layer(edge_tensor), dtype=tf.float32)\n",
        "\n",
        "        node_tensor *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        edge_tensor *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "\n",
        "        edges = self.edge_layer(tf.add(edge_tensor, label_tensor))\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.enc_layers):\n",
        "            if i == 0:\n",
        "                x = self.enc_layers[i][0](node_tensor)\n",
        "                x = self.enc_layers[i][1](x, self.trainable)\n",
        "                x += edges \n",
        "            else:\n",
        "                shortcut = x\n",
        "                x = self.enc_layers[i][0](x)\n",
        "                x = self.enc_layers[i][1](x, self.trainable)\n",
        "                x += edges\n",
        "                x += shortcut\n",
        "\n",
        "        return self.layernorm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL2DMEnzKWZ0",
        "colab_type": "text"
      },
      "source": [
        "# Implement RNN Decoder Module Layers\n",
        "To assemble the Decoder module, we will use the following layers:\n",
        "- Embedding Layer\n",
        "- Bidirectional GRU\n",
        "- Bahanadu Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIP59mUwz4Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN Decoder =  embedding + birirect_GRU + BahanaduAttention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QMzcUBCKp6r",
        "colab_type": "text"
      },
      "source": [
        "### Define Bahdanau Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SaYDTyv0s3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bahanadu Attention layer\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, hidden_size)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi0Pcla9Kv1n",
        "colab_type": "text"
      },
      "source": [
        "### Assemble RNN decoder Module\n",
        "We combine all the decoder layers under one class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZzPGru8z5wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN Decoder\n",
        "class RNNDecoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.forward_gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    return_state=True,\n",
        "                                                    go_backwards=False,\n",
        "                                                    recurrent_initializer='glorot_uniform')\n",
        "        self.backward_gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                                     return_sequences=True,\n",
        "                                                     return_state=True,\n",
        "                                                     go_backwards=True,\n",
        "                                                     recurrent_initializer='glorot_uniform')\n",
        "        self.gru = tf.keras.layers.Bidirectional(self.forward_gru, backward_layer=self.backward_gru,\n",
        "                                                 merge_mode='ave')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1) , x], axis=-1) \n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output = self.gru(x)\n",
        "        output, state = output[0], output[2]\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        output = self.layernorm(output)\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = tf.nn.softmax(self.fc(output))\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soznaypPLThj",
        "colab_type": "text"
      },
      "source": [
        "# Implement GAT Model as Generator\n",
        "We put together the encoder and decoder modules from above to construct the Graph Attention model:\n",
        "\n",
        "GAT model gets input tensors (nodes, labels, node1, node2) and state (generated sequence uptil time t), and will output the probabilits of next token in sequence, as well as the decoder's hidden state.\n",
        "\n",
        "The Encoder recieves the input tensors and uses the Graph Attention module to encode them. The Decoder will then be fed the current state of the generated sequence, along with the encoder hidden state and the encoder output the next token in the sequence, the decoder rnn state, and the attention weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajxMqJ8r09hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GATModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Model that uses Graph Attention encoder and RNN decoder (for now)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 enc_layers, enc_units,  emb_dim, num_heads,\n",
        "                 hidden_size, filter_size, batch_size,  reg_scale,\n",
        "                 dropout, src_vocab_size, tgt_vocab_size,\n",
        "                 target_lang):\n",
        "      \n",
        "        super(GATModel, self).__init__()\n",
        "        self.emb_layer = EmbeddingSharedWeights(\n",
        "            src_vocab_size, emb_dim)\n",
        "\n",
        "        self.tgt_emb_layer = EmbeddingSharedWeights(\n",
        "            tgt_vocab_size, emb_dim)\n",
        "\n",
        "        self.encoder = GraphEncoder(enc_layers, emb_dim, num_heads, True,  hidden_size, # trainable=True\n",
        "                                    filter_size, reg_scale=reg_scale, rate=dropout)\n",
        "        self.decoder = RNNDecoder(tgt_vocab_size, emb_dim, enc_units, batch_size)\n",
        "        self.vocab_tgt_size = tgt_vocab_size\n",
        "        self.batch_size=batch_size\n",
        "        self.num_heads = num_heads\n",
        "        self.target_lang = target_lang\n",
        "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        self.hidden = tf.keras.layers.Dense(hidden_size)\n",
        "\n",
        "\n",
        "    def __call__(self, nodes, labels, node1, node2, state):\n",
        "        \"\"\"\n",
        "        Puts the tensors through encoders and decoders\n",
        "        :param adj: Adjacency matrices of input example\n",
        "        :type adj: tf.tensor\n",
        "        :param nodes: node features\n",
        "        :type nodes: tf.tensor\n",
        "        :param targ: target sequences\n",
        "        :type targ: tf.tensor\n",
        "        :return: output probability distribution\n",
        "        :rtype: tf.tensor\n",
        "        \"\"\"\n",
        "        node_tensor = tf.cast(self.emb_layer(nodes), dtype=tf.float32)\n",
        "        label_tensor = tf.cast(self.emb_layer(labels), dtype=tf.float32)\n",
        "        node1_tensor = tf.cast(self.emb_layer(node1), dtype=tf.float32)\n",
        "        node2_tensor = tf.cast(self.emb_layer(node2), dtype=tf.float32)\n",
        "        #reward = tf.cast(self.emb_layer(reward), dtype=tf.float32)\n",
        "\n",
        "        enc_output = self.encoder(node_tensor, label_tensor, node1_tensor, node2_tensor) # self.num_heads, self.encoder.trainable\n",
        "        batch = enc_output.shape[0]\n",
        "        self.enc_output_hidden = tf.reshape(enc_output, shape=[batch, -1])\n",
        "        enc_hidden = self.hidden(self.enc_output_hidden)\n",
        "\n",
        "\n",
        "        predictions, dec_hidden, _ = self.decoder(state, enc_hidden, enc_output)\n",
        "\n",
        "        return predictions, dec_hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgKqm77ruBaS",
        "colab_type": "text"
      },
      "source": [
        "# Implement LSTM model as Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLEOkDm5uAso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Lambda,  Concatenate, LSTM\n",
        " \n",
        "from tensorflow.keras.layers import Dense, Embedding,Activation, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def Discriminator(V, E, H=64, dropout=0.1):\n",
        "    '''\n",
        "    Disciriminator model.\n",
        "    # Arguments:\n",
        "        V: int, Vocabrary size\n",
        "        E: int, Embedding size\n",
        "        H: int, LSTM hidden size\n",
        "        dropout: float\n",
        "    # Returns:\n",
        "        discriminator: keras model\n",
        "            input: word ids, shape = (B, T)\n",
        "            output: probability of true data or not, shape = (B, 1)\n",
        "    '''\n",
        "\n",
        "    def Highway(x, num_layers=1, activation='relu', name_prefix=''):\n",
        "\n",
        "      '''\n",
        "      Layer wrapper function for Highway network\n",
        "      # Arguments:\n",
        "          x: tensor, shape = (B, input_size)\n",
        "      # Optional Arguments:\n",
        "          num_layers: int, dafault is 1, the number of Highway network layers\n",
        "          activation: keras activation, default is 'relu'\n",
        "          name_prefix: str, default is '', layer name prefix\n",
        "      # Returns:\n",
        "          out: tensor, shape = (B, input_size)\n",
        "      '''\n",
        "      input_size = K.int_shape(x)[1]\n",
        "      for i in range(num_layers):\n",
        "          gate_ratio_name = '{}Highway/Gate_ratio_{}'.format(name_prefix, i)\n",
        "          fc_name = '{}Highway/FC_{}'.format(name_prefix, i)\n",
        "          gate_name = '{}Highway/Gate_{}'.format(name_prefix, i)\n",
        "\n",
        "          gate_ratio = Dense(input_size, activation='sigmoid', name=gate_ratio_name)(x)\n",
        "          fc = Dense(input_size, activation=activation, name=fc_name)(x)\n",
        "          x = Lambda(lambda args: args[0] * args[2] + args[1] * (1 - args[2]), name=gate_name)([fc, x, gate_ratio])\n",
        "      return x\n",
        "\n",
        "\n",
        "    input_ = Input(shape=(None,), dtype='int32', name='Input')   # (B, T)\n",
        "    out = Embedding(V, E, mask_zero=True, name='Embedding')(input_)  # (B, T, E)\n",
        "    out = LSTM(H)(out)\n",
        "    out = Highway(out, num_layers=1)\n",
        "    out = Dropout(dropout, name='Dropout')(out)\n",
        "    out = Dense(1, activation='sigmoid', name='FC')(out)\n",
        "\n",
        "    discriminator = Model(input_, out)\n",
        "    return discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I1Katv0dRpH",
        "colab_type": "text"
      },
      "source": [
        "# Define generator loss, Agent and Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX6HOehAcofg",
        "colab_type": "text"
      },
      "source": [
        "## Define Agent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmwh0MMDT4d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    '''\n",
        "    On each step, Agent act on state.\n",
        "    Then Environment return next state, reward, and so on.\n",
        "    '''\n",
        "    def __init__(self, BATCH_SIZE, src_vocab_size, generator_model, lr=1e-3):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            BATCH_SIZE: int, batch_size\n",
        "            src_vocab_size: int, Vocabrary size\n",
        "            emb_dim: int, Embedding size\n",
        "        # Optional Arguments:\n",
        "            lr: float, learning rate, default is 0.001\n",
        "        '''\n",
        "        self.num_actions = src_vocab_size\n",
        "        self.B = BATCH_SIZE\n",
        "        self.V = src_vocab_size\n",
        "        self.lr = lr\n",
        "        self.eps = 0.1\n",
        "        self.generator = generator_model\n",
        "\n",
        "    def act(self, nodes, labels, node1, node2, state, epsilon=0, deterministic=False):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            state: numpy array, dtype=int, shape = (B, t)\n",
        "            epsilon: float, 0 <= epsilon <= 1,\n",
        "                if epsilon is 1, the Agent will act completely random.\n",
        "        # Returns:\n",
        "            action: numpy array, dtype=int, shape = (B, 1)\n",
        "        '''\n",
        "        try:\n",
        "            word = state.numpy()#[:, -1].reshape([self.B, 1], dtype=np.int32)\n",
        "        except AttributeError as e:\n",
        "            word =np.expand_dims( state[:,-1] , axis=1)\n",
        "            pass\n",
        "        \n",
        "\n",
        "        return self._act_on_word(nodes, labels, node1, node2, word, epsilon=epsilon, deterministic=deterministic)\n",
        "\n",
        "\n",
        "    def prepare_prob(self, x):\n",
        "          x = np.array(x)\n",
        "          tot = sum(x)\n",
        "          if tot > 1.0:\n",
        "              leftover = tot - 1.0 \n",
        "              x[-1] =  x[-1] - leftover\n",
        "          elif tot < 1.0:\n",
        "              leftover = 1.0 - tot \n",
        "              x[-1] =  x[-1] + leftover\n",
        "          return x\n",
        "\n",
        "\n",
        "    def sampling_word(self, prob):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            prob: numpy array, dtype=float, shape = (B, V),\n",
        "        # Returns:\n",
        "            action: numpy array, dtype=int, shape = (B, )\n",
        "        '''\n",
        "        action = np.zeros((self.B,), dtype=np.int32)\n",
        "        for i in range(self.B):\n",
        "            p = prob[i]\n",
        "            p = self.prepare_prob(p)\n",
        "\n",
        "            action[i] = np.random.choice(self.V, p=p)\n",
        "        return action\n",
        "\n",
        "    def _act_on_word(self, nodes, labels, node1, node2, word, epsilon=0, deterministic=False, PAD=0, EOS=8):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            word: numpy array, dtype=int, shape = (B, 1),\n",
        "                word indicates current word.\n",
        "            epsilon: float, 0 <= epsilon <= 1,\n",
        "                if epsilon is 1, the Agent will act completely random.\n",
        "        # Returns:\n",
        "            action: numpy array, dtype=int, shape = (B, 1)\n",
        "        '''\n",
        "        action = None\n",
        "        is_PAD = word == PAD\n",
        "        is_EOS = word == EOS\n",
        "        is_end = is_PAD.astype(np.int) + is_EOS.astype(np.int)\n",
        "        is_end = 1 - is_end\n",
        "\n",
        "        is_end = is_end.reshape([self.B, 1])\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = np.random.randint(low=0, high=self.num_actions, size=(self.B, 1))\n",
        "            return epsilon, action * is_end\n",
        "        elif not deterministic:\n",
        "            probs, dec_hidden = self.generator(nodes, labels, node1, node2, word)\n",
        "            action = self.sampling_word(probs).reshape([self.B, 1])\n",
        "        else:\n",
        "            probs = self.generator(nodes, labels, node1, node2, word) # (B, T)\n",
        "            action = np.argmax(probs, axis=-1).reshape([self.B, 1])\n",
        "\n",
        "        return probs, action * is_end\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5x4EslBctTA",
        "colab_type": "text"
      },
      "source": [
        "## Define Environment class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoubqrzT4rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment(object):\n",
        "    '''\n",
        "    On each step, Agent act on state.\n",
        "    Then Environment return next state, reward, and so on.\n",
        "    '''\n",
        "    def __init__(self, discriminator, agent_copy, n_sample=5):\n",
        "        '''\n",
        "        Environment class for Reinforcement Learning\n",
        "        # Arguments:\n",
        "            discriminator: keras model\n",
        "            g_beta: SeqGAN.rl.Agent, copy of Agent\n",
        "                params of g_beta.generator should be updated with those of original\n",
        "                generator on regular occasions.\n",
        "        # Optional Arguments\n",
        "            n_sample: int, default is 16, the number of Monte Calro search sample\n",
        "        '''\n",
        "\n",
        "        self.n_sample = n_sample\n",
        "        self.BOS = 7\n",
        "        self.discriminator = discriminator\n",
        "        self.g_beta = agent_copy\n",
        "        self.B = batch_size\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 1\n",
        "        self._state = np.zeros([self.B, 1], dtype=np.int32)\n",
        "        self._state[:, 0] = self.BOS\n",
        "\n",
        "\n",
        "\n",
        "    def get_state(self):\n",
        "        return self._state     #[:, 1:]   # Exclude BOS\n",
        "\n",
        "    def _append_state(self, word, state=None):\n",
        "        '''\n",
        "        # Arguments:\n",
        "            word: numpy array, dtype=int, shape = (B, 1)\n",
        "        '''\n",
        "\n",
        "        if state is None:\n",
        "\n",
        "            #print(word)\n",
        "            #print(self._state)\n",
        "            self._state = np.concatenate([self._state, word], axis=-1)\n",
        "        else:\n",
        "\n",
        "            return np.concatenate([state, word], axis= -1)\n",
        "\n",
        "    def step(self, nodes, labels, node1, node2, action, targ_shape):\n",
        "        '''\n",
        "        Step t -> t + 1 and returns a result of the Agent action.\n",
        "        # Arguments:\n",
        "            action: numpy array, dtype=int, shape = (B, 1),\n",
        "                state is Y_0:t-1, and action is y_t\n",
        "        # Returns:\n",
        "            next_state: numpy array, dtype=int, shape = (B, t)\n",
        "            reward: numpy array, dtype=float, shape = (B, 1)\n",
        "            is_episode_end: bool\n",
        "            info: dict\n",
        "        '''\n",
        "        self.t = self.t + 1\n",
        "\n",
        "        reward = self.Q(nodes, labels, node1, node2, action, self.n_sample, targ_shape)\n",
        "        is_episode_end = self.t > targ_shape\n",
        "\n",
        "        self._append_state(action)\n",
        "        next_state = self.get_state()\n",
        "        info = None\n",
        "\n",
        "        return [next_state, reward, is_episode_end, info]\n",
        "\n",
        "    def render(self, head=1):\n",
        "        for i in range(head):\n",
        "            ids = self.get_state()[i]\n",
        "            words = [self.data_generator.id2word[id] for id in ids.tolist()]\n",
        "            print(''.join(words))\n",
        "        print('-' * 80)\n",
        "\n",
        "\n",
        "    def Q(self, nodes, labels, node1, node2, action, targ_shape, n_sample=16):\n",
        "        '''\n",
        "        State-Action value function using Rollout policy\n",
        "        # Arguments:\n",
        "            action: numpy array, dtype=int, shape = (B, 1)\n",
        "\n",
        "        # Optional Arguments:\n",
        "            n_sample: int, default is 16, number of samples for Monte Calro Search\n",
        "\n",
        "        # Returns:\n",
        "            reward: numpy array, dtype=float, shape = (B, ), State-Action value\n",
        "\n",
        "        # Requires:\n",
        "            t, T: used to define time range.\n",
        "            state: determined texts, Y[0:t-1], used for Rollout.\n",
        "            action: next words, y[t], used for sentence Y[0:t].\n",
        "            g_beta: Rollout policy.\n",
        "        '''\n",
        " \n",
        "        reward = np.zeros([self.B, 1])\n",
        "        if self.t == 2:\n",
        "            Y_base = self._state    # Initial case\n",
        "        else:\n",
        "            Y_base = self.get_state()    # (B, t-1)\n",
        "\n",
        "        if self.t >= targ_shape:\n",
        "            Y = self._append_state(action, state=Y_base)\n",
        "            return self.discriminator.predict(Y)\n",
        "\n",
        "        # Rollout\n",
        "        for idx_sample in range(n_sample):\n",
        "            Y = Y_base\n",
        "\n",
        "            _, y_t = self.g_beta.act(nodes, labels, node1, node2, Y, epsilon=self.g_beta.eps)\n",
        "\n",
        "            Y = self._append_state(y_t, state=Y)\n",
        "            \n",
        "            for tau in range(self.t+1, targ_shape):\n",
        "                _, y_tau = self.g_beta.act(nodes, labels, node1, node2, Y, epsilon=self.g_beta.eps)\n",
        "                Y = self._append_state(y_tau, state=Y)   \n",
        "            reward += self.discriminator.predict(Y) / n_sample\n",
        "            #print('generated : ', self._append_state(y_tau, state=Y))\n",
        "            #print('reward : ', reward)\n",
        "\n",
        "        return reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wI4Z03c2cd",
        "colab_type": "text"
      },
      "source": [
        "## Initialize training variables and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMCFj5k-lSLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10248\n",
        "embedding_size = 16\n",
        "BATCH_SIZE = batch_size\n",
        "epochs = 10\n",
        "step=0\n",
        "steps = epochs * steps_per_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d95fc6ahjsLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "GATModel args: \n",
        "\n",
        "    enc_layers, enc_units,  emb_dim, num_heads,\n",
        "    hidden_size, filter_size, batch_size,  reg_scale,\n",
        "    dropout, src_vocab_size, tgt_vocab_size,\n",
        "    target_lang\n",
        "\n",
        "'''\n",
        "\n",
        "# Generator model\n",
        "generator = GATModel(2, 64,  64, 2,\n",
        "                 64, 64, BATCH_SIZE, 0.0,\n",
        "                 0.2, src_vocab_size, tgt_vocab_size,\n",
        "                 tgt_vocab)\n",
        "\n",
        "# Generator optimizer\n",
        "gen_optimizer = tf.keras.optimizers.Adam()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_LJh7w2Wf_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Generator checkpoint/not necessary for discriminator##################\n",
        "\n",
        "\n",
        "checkpoint_dir = './gdrive/My Drive/RDF_GAN/training_checkpoints'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUSN86MxlErb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Discriminator model\n",
        "discriminator = Discriminator(vocab_size,\n",
        "                              embedding_size,\n",
        "                              H=64, dropout=0.1).compile('adam', 'binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdPBeclc5ift",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator(vocab_size,\n",
        "                              embedding_size,\n",
        "                              H=64, dropout=0.1)\n",
        "discriminator.compile('adam', 'binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Ka5THDlE39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agent instance, takes initialized generator\n",
        "agent = Agent(BATCH_SIZE, vocab_size, generator)\n",
        "\n",
        "\n",
        "# Environment instance, takes initialized discriminator and generator\n",
        "environment =  Environment(discriminator, agent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2PZbsKjiM0H",
        "colab_type": "text"
      },
      "source": [
        "## Define function to generate discriminator data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIxua4laiKwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_disc_data(real, fake):\n",
        "\n",
        "    real.extend(fake[0])\n",
        "    random.shuffle(real)\n",
        "\n",
        "    disc_data = real\n",
        "    disc_data = np.vstack(disc_data)\n",
        "    \n",
        "    X, y = disc_data[:,:-1], disc_data[:,-1]\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8k_nyQpiBRs",
        "colab_type": "text"
      },
      "source": [
        "## Define Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO9RhfTI9enI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(y_pred, y_true, reward):\n",
        "\n",
        "    out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
        "\n",
        "    y_true = K.one_hot(y_true, num_classes=vocab_size)\n",
        "\n",
        "    log_lik = y_true*K.log(out)\n",
        "    \n",
        "    return K.sum(-log_lik * reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7XVb6UtSAuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_output(state, targ):\n",
        "\n",
        "\n",
        "  print('\\nReal : ')\n",
        "  for t in targ:\n",
        "    print(' '.join(['<PAD>' if i==0 else tgt_vocab.index_word[i] for i in np.array(t)]))\n",
        "    break\n",
        "    \n",
        "  print('\\nGenerated : ')\n",
        "  for s in state:\n",
        "    print(' '.join(['<PAD>' if i==0 else tgt_vocab.index_word[i] for i in s]), '\\n')\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKXz_ztSgsET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_progress(path):\n",
        "  agent.generator.save_weights(path+'/generator_weights')\n",
        "  discriminator.save_weights(path+'/discriminator_weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiEEXXBihUhA",
        "colab_type": "text"
      },
      "source": [
        "## Define training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOqbn43JhRSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Training step \n",
        "#@tf.function\n",
        "def train_step(nodes, labels, node1, node2, targ, agent, environment, discriminator):\n",
        "  loss = 0\n",
        "  st = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Lists to store training data for discriminator\n",
        "    disc_fake = []\n",
        "\n",
        "    # Store the target sequences as a negative class instance \n",
        "    disc_real = [[np.concatenate([np.array(y), np.array([1])])] for y in targ]\n",
        "\n",
        "    # predictions with Rollout policy, using environment class\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      st+=1\n",
        "      if st%10==0:\n",
        "          print('\\nt-Step :', st)\n",
        "\n",
        "\n",
        "      # Get state at time t\n",
        "      state = targ[:,t-1 :t]\n",
        "\n",
        "      # Get predictions and action from agent\n",
        "      probs, action = agent.act(nodes, labels, node1, node2, state)\n",
        "\n",
        "      # Generate next state and reward from environment\n",
        "      state_, reward, is_episode_end, info = environment.step(nodes, labels, node1, node2, action, targ.shape[1])\n",
        "\n",
        "      # Calculate loss\n",
        "      \n",
        "      #print(probs.shape, K.one_hot(targ[:,t], num_classes=10013).shape)\n",
        "      loss += generator_loss(probs, targ[:,t], reward)#action\n",
        "\n",
        "\n",
        "    # generate output to examine\n",
        "    generate_output(state_, targ)\n",
        "\n",
        "    # Store the complete generated sequences as a negative class instance \n",
        "    disc_fake.append([[np.concatenate([np.array(s), np.array([0])])] for s in state_])\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    # Get X_train, y_train for discriminator\n",
        "    d_x, d_y = get_disc_data(disc_real, disc_fake)\n",
        "\n",
        "    # Train Discriminator\n",
        "    print('Discriminator loss :')\n",
        "    discriminator.fit(d_x, d_y, batch_size=BATCH_SIZE, epochs=1)\n",
        "\n",
        "  # Calculate batch loss\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  # Define trainable variables\n",
        "  variables = agent.generator.trainable_variables\n",
        "  \n",
        "  # Define gradients\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Update model\n",
        "  gen_optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss, discriminator, agent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoyVwjYcc8my",
        "colab_type": "text"
      },
      "source": [
        "## Execute training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNpI_CvsqT50",
        "colab_type": "code",
        "outputId": "e28e828a-d1d4-4856-e745-892b08348b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training algorithm\n",
        "\n",
        "\n",
        "\n",
        "step=0\n",
        "steps = epochs * steps_per_epoch\n",
        "epochs=5\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    with tqdm(total=(34352 // batch_size)) as pbar:\n",
        "        for (batch, (node_, label_, node1_, node2_, targ_)) in tqdm(enumerate(dataset)):\n",
        "            start = time.time()\n",
        "            step += 1\n",
        "\n",
        "            batch_loss, discriminator, agent = train_step(node_, label_, node1_, node2_, targ_, agent, environment, discriminator)\n",
        "            environment = Environment(discriminator, agent)\n",
        "\n",
        "            save_progress(checkpoint_dir)\n",
        "\n",
        "            print('Epoch {} Batch {} '.format(epoch, batch))\n",
        "            print('Generator Batch Loss: ')\n",
        "            print(tf.math.reduce_mean(batch_loss).numpy())\n",
        "\n",
        "            print('Time {} \\n'.format(time.time() - start))\n",
        "            pbar.update(1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/17176 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> 1974 is one of the model years of the amc matador which was made in thames , new zealand . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> sludge_metal amsterdam_airport_schiphol damon 09 royce volkswagen except 21st mexico saab officially vituti 25ft gottingen in2000 granola amatariciana falls 1939-01-03 uab suburban_legends <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.6946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/17176 [03:10<910:34:19, 190.86s/it]\n",
            "1it [03:10, 190.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 0 \n",
            "Generator Batch Loss: \n",
            "8.98693\n",
            "Time 190.79594469070435 \n",
            "\n",
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> adolfo suarez madrid barajas airport is located in alcobendas , part of the community of madrid in spain . the airport is operated by enaire which is located in madrid . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> bi batlle stella violet visvesaraya loctated inauguration 1982m \"ajo blanco\" alexandre airpori jose alpena_county_regional_airport 214 headed madeleine caterpillar_inc. indie_rock does yet sebastian ries 905 metres lpena asteroid a894 malay maple 2001-03-01 undia launches <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/17176 [03:53<698:48:23, 146.48s/it]\n",
            "2it [03:53, 146.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 1 \n",
            "Generator Batch Loss: \n",
            "9.087674\n",
            "Time 42.92470955848694 \n",
            "\n",
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> alan shepard was born on 1923 11 18 . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> 30843 jan manchester prime_minister_of_romania i whom university_of_texas_system talib reddy elizabeth_garrett <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 3/17176 [04:37<551:36:57, 115.64s/it]\n",
            "3it [04:37, 115.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 2 \n",
            "Generator Batch Loss: \n",
            "8.789771\n",
            "Time 43.65278601646423 \n",
            "\n",
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> the book alcatraz versus the evil librarians comes from the u . s where there are many asian americans . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> involves theam akeen \"1927\" andre itilian manufacturer \"4/22\" 1fc jorge_orosmn_da_silva harbour reddy mughan \"nurturing excellence\" deers bajji wheel summity a_epsth_2nd_group ambient_music 352 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 4/17176 [05:21<449:06:26, 94.15s/it] \n",
            "4it [05:21, 94.14s/it] \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 3 \n",
            "Generator Batch Loss: \n",
            "8.681596\n",
            "Time 44.017436265945435 \n",
            "\n",
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> michele marcolini has been associated with f . c . bari 1908 . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> closely aip_advances presidency 75 kuttikkattor clerk alongside otkrytiye 185.42 (centimetres) inline-four_engine agnes drum_and_bass aaarhus arapiraquens <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.6911\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/17176 [06:03<374:28:10, 78.51s/it]\n",
            "5it [06:03, 78.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 4 \n",
            "Generator Batch Loss: \n",
            "8.497562\n",
            "Time 42.001798152923584 \n",
            "\n",
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> the comic character , arion , is also known by he name ahri ahn . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> alcatraz_versus_the_scriveners_bones native albennie_jones rq masum compostela organization 1996-05-30 informally informally andreas_vokuhle filipinos_in_japan zaragoza spaceport lasalle medicine <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/17176 [06:47<325:09:19, 68.17s/it]\n",
            "6it [06:47, 68.17s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 5 \n",
            "Generator Batch Loss: \n",
            "8.392691\n",
            "Time 44.05031108856201 \n",
            "\n",
            "\n",
            "t-Step : 10\n",
            "\n",
            "t-Step : 20\n",
            "\n",
            "t-Step : 30\n",
            "\n",
            "t-Step : 40\n",
            "\n",
            "t-Step : 50\n",
            "\n",
            "t-Step : 60\n",
            "\n",
            "t-Step : 70\n",
            "\n",
            "t-Step : 80\n",
            "\n",
            "Real : \n",
            "<start> the atlas ii , from the u . s . , was made by lockheed martin and launched from the spaceport florida launch complex 36 . <end> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Generated : \n",
            "<start> involved desserts 1 straus coritiba_foot_ball_club here techsystems acoustic political madrid orfeo england serkey rule yemi sroke 1856-09-22 dalpatbhai seattle genada parliament_of_catalonia riverside skyrypnyk utterar uttar a_glastonbury_romance 77 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
            "\n",
            "Discriminator loss :\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.6894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 7/17176 [07:31<290:13:16, 60.85s/it]\n",
            "7it [07:31, 60.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Batch 6 \n",
            "Generator Batch Loss: \n",
            "8.373082\n",
            "Time 43.76064896583557 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FQ0OqXhAaOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOS = tgt_vocab.word_index['<start>']\n",
        "EOS = tgt_vocab.word_index['<end>']\n",
        "\n",
        "BOS, EOS\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFepvYkoDMSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ZthhG1Ges8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiPOGnd66g9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
