{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "experimental_generator_pretraining.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiloyPurkait/GSoC-2020/blob/master/V2.0/Transformers/experimental_generator_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "## Pre-train Generator before adverserial training\n",
        "\n",
        "The generator is pretrained using teacher-forcing. \n",
        "\n",
        "- Adapted from : https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpEPUovEETH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install tf-nightly-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "from re import finditer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XR8J2UYJYZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "file_path = \"/content/gdrive/My Drive/f_data.txt\"\n",
        "test_path = \"/content/gdrive/My Drive/data/processed_graphs/eng/gat/test_data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSKfG9NNogqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pretraining import *\n",
        "from transformer_generator import *\n",
        "from transformer_discriminator import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkXk7ZMDZ-w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_dataset, tokenizer_txt = create_generator_dataset(file_path, BATCH_SIZE=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbX9_UgBb_jP",
        "colab_type": "text"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u3D1CV-buTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    real_loss = loss_object(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = loss_object(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "#Primary loss for plain adverserial training\n",
        "def generator_loss(real_output, fake_output):\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    loss_ = loss_object(tf.ones_like(fake_output), fake_output)\n",
        "    return  loss_ #tf.reduce_sum(\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "colab": {}
      },
      "source": [
        "#Generator params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "\n",
        "target_vocab_size = tokenizer_txt.vocab_size + 2\n",
        "input_vocab_size = target_vocab_size\n",
        "dropout_rate = 0.1\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7r4scdulztRx",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxEs9TvYov9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJMjiUvB_2TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain_loss_function(real, pred):\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  \n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "def pretrain_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = generator(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    \n",
        "    loss = pretrain_loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, generator.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUd_1x1OBeXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for (inpt, targ) in train_dataset:\n",
        "  pretrain_step(inpt, targ)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRg47HNvBgbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.load_weights('./generator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrTP--5o7UyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_MAX_LEN = 250\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator = TransformerDiscriminator(tokenizer_txt.vocab_size+2, maxlen=DATA_MAX_LEN)\n",
        "discriminator.load_weights('./discriminator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./content/checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator=generator,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV7plWRg615x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LKpoA6q1sJFj",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKONxqc2u7Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render_preds(batch_pred, inp, tar, n=2):\n",
        "    print(type(batch_pred), type(inp), batch_pred.shape, inp.shape)\n",
        "    for (ind,i) in enumerate(batch_pred):\n",
        "      print('\\n| Predicted: ', decode_text(i, tokenizer_txt))\n",
        "      print('| True: ', decode_text(tar[ind], tokenizer_txt))\n",
        "      print('| Input RDF: ', decode_text(inp[ind], tokenizer_txt))\n",
        "      print()\n",
        "      if ind==n:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAfIjxpvzhWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##max_len global varable\n",
        "\n",
        "def gen_batch(preds, inp, tar, max_len = 100):\n",
        "\n",
        "  disc_data = []\n",
        "  for sent in preds:\n",
        "    unparsed = decode_text(sent, tokenizer_txt)\n",
        "    retokenized = tokenizer_txt.encode(unparsed.split('<end>')[0]+'<end>')\n",
        "    padded = np.pad(np.array(retokenized), (0, max_len - len(retokenized)), 'constant')\n",
        "\n",
        "    disc_data.append(padded)\n",
        "  disc_data = tf.convert_to_tensor(disc_data, dtype=inp.dtype)\n",
        "  gens = tf.concat([inp, disc_data], axis=-1, name='concat')\n",
        "  real = tf.concat([inp, tar], axis=-1, name='concat')\n",
        "\n",
        "  return gens, real\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb1TEDcTJJYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def pad(tensor, maxlen=250):\n",
        "   return tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                        padding='post',\n",
        "                                                        value=0,\n",
        "                                                        maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5JkvYmTLiSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator.trainable_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJwmp9OE29oj",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "#@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as gen_tape:#, tf.GradientTape() as disc_tape:\n",
        "\n",
        "    predictions, _ = generator(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    \n",
        "\n",
        "    \n",
        "    batch_pred = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    generated, real = gen_batch(batch_pred, inp, tar)\n",
        "    generated, real = pad(generated), pad(real)\n",
        "\n",
        "    real_output = discriminator(real, training=True)\n",
        "    fake_output = discriminator(generated, training=True)\n",
        "    \n",
        "    gen_loss = generator_loss(real_output, fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    #print(gen_loss)\n",
        "    #print(disc_loss)\n",
        "\n",
        "  gen_tape.watch(gen_loss)\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  print(gradients_of_generator)\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  print(gradients_of_discriminator)\n",
        "  print(len(gradients_of_generator))\n",
        "  print(len(gradients_of_discriminator))\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "  \n",
        "  train_loss(gen_loss)\n",
        "  train_loss(disc_loss)\n",
        "  train_accuracy(tar_real, predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89dW5ns-FZCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbvmaKNiznHZ",
        "colab": {}
      },
      "source": [
        "if train:\n",
        "  \n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "      train_step(inp, tar)\n",
        "      \n",
        "      if batch % 50 == 0:\n",
        "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                          ckpt_save_path))\n",
        "      \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                  train_loss.result(), \n",
        "                                                  train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKNsUR03iYGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights('./generator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-3kov6Dc6LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_(inp_sentence):\n",
        "\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "  decoder_input = [tokenizer_txt.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_txt.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0)\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FahpXe9at5nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH=250\n",
        "rdfb, txtb = next(iter(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZGs83toe6HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentence = evaluate_(rdfb[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbPUmxKcUHgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_text(predicted_sentence, tokenizer_txt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}