{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Adversarial Training (with error).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiloyPurkait/GSoC-2020/blob/master/V2.0/Transformers/Adversarial_Training(with_error).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "## Adverserial training script\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpEPUovEETH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! pip install tf-nightly-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "from re import finditer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XR8J2UYJYZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "file_path = \"/content/gdrive/My Drive/f_data.txt\"\n",
        "test_path = \"/content/gdrive/My Drive/data/processed_graphs/eng/gat/test_data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSKfG9NNogqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pretraining import *\n",
        "from transformer_generator import *\n",
        "from transformer_discriminator import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkXk7ZMDZ-w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 16\n",
        "max_len = 40\n",
        "train_dataset, tokenizer_txt = create_generator_dataset(file_path, BATCH_SIZE=batch_size, MAX_LEN=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbX9_UgBb_jP",
        "colab_type": "text"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u3D1CV-buTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "\n",
        "    '''\n",
        "  Quantifies discriminator's ability to distinguish real sequences from fakes.\n",
        "  It compares the discriminator's predictions on real images to an array of 1s,\n",
        "  and the discriminator's predictions on fake (generated) sequences\n",
        "  to an array of 0s.\n",
        "    '''\n",
        "\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    real_loss = loss_object(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = loss_object(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "def generator_loss(real_output, fake_output):\n",
        "\n",
        "    '''\n",
        "  Quantifies generator's ability to trick the discriminator. \n",
        "  If the generator is doing well, discriminator will classify \n",
        "  fake sequences as real (or 1). We thus compare the discriminators\n",
        "  decisions on the generated sequences to an array of 1s.\n",
        "    '''\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    fake_output = tf.convert_to_tensor(fake_output, dtype=tf.float32)\n",
        "    loss_ = loss_object(tf.ones_like(fake_output,dtype=tf.float32), fake_output)\n",
        "    return  loss_ #tf.reduce_sum(\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "colab": {}
      },
      "source": [
        "#Generator params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = target_vocab_size = tokenizer_txt.vocab_size + 2\n",
        " \n",
        "dropout_rate = 0.1\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7r4scdulztRx",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxEs9TvYov9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJMjiUvB_2TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrain_loss_function(real, pred):\n",
        "  '''\n",
        "  # Sparse categorical crossentropy \n",
        "  # loss function used for generator pretraining\n",
        "  '''\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  \n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "def pretrain_step(inp, tar):\n",
        "  '''\n",
        "  # Pretraining step for generator network\n",
        "  '''\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = generator(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    \n",
        "    loss = pretrain_loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, generator.trainable_variables)    \n",
        "  generator_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbGEv0aVxlWT",
        "colab_type": "text"
      },
      "source": [
        "## Pass data through generator to be able to load in weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUd_1x1OBeXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for (inpt, targ) in train_dataset:\n",
        "  pretrain_step(inpt, targ)\n",
        "  break\n",
        "generator.load_weights('./generator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IeRv0tnxsA5",
        "colab_type": "text"
      },
      "source": [
        "## Define discriminator and load in weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrTP--5o7UyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define discriminator and load in weights\n",
        "DATA_MAX_LEN = 250\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator = TransformerDiscriminator(tokenizer_txt.vocab_size+2, maxlen=DATA_MAX_LEN)\n",
        "discriminator.load_weights('./discriminator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "## Create the checkpoint path and the checkpoint manager.\n",
        " This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./content/checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(generator=generator,\n",
        "                           optimizer=generator_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV7plWRg615x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4mXkYbxzlS",
        "colab_type": "text"
      },
      "source": [
        "## Define helper functions to print print and pad predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKONxqc2u7Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def render_preds(batch_pred, inp, tar, n=2):\n",
        "    '''\n",
        "    Print out input, target, and preds\n",
        "    '''\n",
        "    print(type(batch_pred), type(inp), batch_pred.shape, inp.shape)\n",
        "    for (ind,i) in enumerate(batch_pred):\n",
        "      print('\\n| Predicted: ', decode_text(i, tokenizer_txt))\n",
        "      print('| True: ', decode_text(tar[ind], tokenizer_txt))\n",
        "      print('| Input RDF: ', decode_text(inp[ind], tokenizer_txt))\n",
        "      print()\n",
        "      if ind==n:\n",
        "        break\n",
        "\n",
        "def pad(tensor, maxlen=250):\n",
        "    '''\n",
        "    Used to pad a tensor, such as a batch of generated text\n",
        "    '''\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                        padding='post',\n",
        "                                                        value=0,\n",
        "                                                        maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onh3tWPrx58G",
        "colab_type": "text"
      },
      "source": [
        "## Define function to clean generator output and concatenate sequences during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAfIjxpvzhWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##max_len global varable\n",
        "\n",
        "\n",
        "def gen_batch(preds, inp, tar):\n",
        "  '''\n",
        "  Cleans predictions and created concatenated \n",
        "  triple-text batches for the discriminator\n",
        "\n",
        "  Takes predictions, input and target batch\n",
        "  Returns training data for discriminator (all_data, all_labels)\n",
        "          as well as the generated triple-text batch (gens)\n",
        "  '''\n",
        "  #to collect cleaned generations\n",
        "  gen_data = []\n",
        "\n",
        "  # Iterate over predictions in batch\n",
        "  for sent in preds:\n",
        "\n",
        "    # Decode predicted sequence into string\n",
        "    unparsed = decode_text(sent, tokenizer_txt)\n",
        "\n",
        "    #  Remove all characters after '<end>' token from generated outputs\n",
        "    retokenized = tokenizer_txt.encode(unparsed.split('<end>')[0]+'<end>')\n",
        "    gen_data.append(retokenized)\n",
        "\n",
        "  # Pad cleaned generations\n",
        "  gen_data = pad(gen_data)\n",
        "\n",
        "  # Horizontally stack input triples and generated sequences\n",
        "  gens = pad(tf.concat([inp, gen_data], axis=-1, name='concat'))\n",
        "\n",
        "  # Horizontally input triples and real target sequences\n",
        "  real = pad(tf.concat([inp, tar], axis=-1, name='concat'))\n",
        "\n",
        "  # Vertically stack real and generated sequences \n",
        "  all_data = tf.concat([gens, real], axis=0)\n",
        "  all_labels = tf.concat(  [\n",
        "                            tf.zeros((batch_size, 1)),\n",
        "                            tf.ones((batch_size, 1))\n",
        "                           ], axis=0)\n",
        "\n",
        "  return all_data, all_labels, gens\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Wrap the function into tensorflow op for eager execution\n",
        "@tf.function( experimental_relax_shapes=True)\n",
        "def tf_gen_batch(preds, inp,  tar):\n",
        "  all_data, all_labels, gens = tf.py_function(gen_batch, inp=[preds, inp, tar], Tout=[tf.int32, tf.float32, tf.int32])\n",
        "  return all_data, all_labels, gens\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnOu_GH1yCwQ",
        "colab_type": "text"
      },
      "source": [
        "## Define training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb9KDzXnYJ_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##\n",
        "## Nested gradient tapes\n",
        "##\n",
        "\n",
        "\n",
        "def train_step(inp, tar):\n",
        "\n",
        "    # targets shifted by 1 index position\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    \n",
        "    #Get encoding, combined and decoding masks\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    # Initialize Generator gradient tape\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "\n",
        "        #Get predictions from generator\n",
        "        predictions, _ = generator(inp, tar_inp, \n",
        "                             True, \n",
        "                             enc_padding_mask, \n",
        "                             combined_mask, \n",
        "                             dec_padding_mask)\n",
        "\n",
        "        #Get predictions per input in batch\n",
        "        batch_pred = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "        # Make discriminator training data by cleaning \n",
        "        # generated dataand assembling real data\n",
        "        all, labels, gens = tf_gen_batch(batch_pred,  inp, tar)\n",
        "\n",
        "        # Add noise to labels\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Get discriminator's predictions of generator's output\n",
        "        disc_preds = discriminator(gens)\n",
        "        disc_preds = tf.convert_to_tensor( disc_preds, dtype=tf.float32)\n",
        "\n",
        "        # Initialize discriminator gradient tape\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "\n",
        "            # predict on real and generated sequence\n",
        "            predictions = discriminator(all)\n",
        "            # Calculate loss using discriminator loss function\n",
        "            d_loss = discriminator_loss(labels, predictions)\n",
        "        \n",
        "        # Get discriminator gradients and apply using optimizer\n",
        "        disc_grads = disc_tape.gradient(d_loss, discriminator.trainable_weights)\n",
        "        discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_weights))\n",
        "    \n",
        "    # Make a tensor of ones, as ideal labels for of generated sequences\n",
        "    ideal_labels = tf.ones((batch_size, 1))\n",
        "    # and calculate generator loss \n",
        "    g_loss = generator_loss(ideal_labels, disc_preds)\n",
        "    \n",
        "    # Get generator gradients and apply using optimizer\n",
        "    gen_grads = gen_tape.gradient(g_loss, generator.trainable_weights)\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_weights))\n",
        "\n",
        "\n",
        "    # display generator loss\n",
        "    train_loss(g_loss)\n",
        "    #display discriminator loss\n",
        "    train_loss(d_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89dW5ns-FZCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH_V-U_qyFiK",
        "colab_type": "text"
      },
      "source": [
        "## Define training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbvmaKNiznHZ",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  '''\n",
        "  Function to initialize training process\n",
        "  Prints Generator and discriminator loss during training\n",
        "  '''\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "      train_step(inp, tar)\n",
        "      \n",
        "      if batch % 50 == 0:\n",
        "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                          ckpt_save_path))\n",
        "      \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                  train_loss.result(), \n",
        "                                                  train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP1pgtUbq0HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL8vl_AVquaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##\n",
        "## Sequential gradient tapes\n",
        "##\n",
        "\n",
        "def train_step(inp, tar):\n",
        "\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "\n",
        "    predictions, _ = generator(inp, tar_inp, \n",
        "                             True, \n",
        "                             enc_padding_mask, \n",
        "                             combined_mask, \n",
        "                             dec_padding_mask)\n",
        "\n",
        "    batch_pred = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "    all, labels, gens = gen_batch(batch_pred, inp, tar)\n",
        "    labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "    disc_preds = discriminator(gens)\n",
        "    disc_preds = tf.convert_to_tensor( disc_preds, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    # Train the discriminator\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(all)\n",
        "        d_loss = discriminator_loss(labels, predictions)\n",
        "\n",
        "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
        "    discriminator_optimizer.apply_gradients(\n",
        "        zip(grads, discriminator.trainable_weights)\n",
        "    )\n",
        "\n",
        "    # Train the generator (note that we should *not* update the weights\n",
        "    # of the discriminator)!\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(disc_preds)\n",
        "\n",
        "        ideal_labels = tf.ones((batch_size, 1))\n",
        "\n",
        "        g_loss = generator_loss(ideal_labels, predictions)\n",
        "    \n",
        "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
        "    print(grads)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "    train_loss(g_loss)\n",
        "    train_loss(d_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F86dYYxOq2IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKNsUR03iYGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.save_weights('./generator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-3kov6Dc6LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_(inp_sentence):\n",
        "\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "  decoder_input = [tokenizer_txt.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_txt.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0)\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FahpXe9at5nV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH=250\n",
        "rdfb, txtb = next(iter(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZGs83toe6HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_sentence = evaluate_(rdfb[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbPUmxKcUHgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_text(predicted_sentence, tokenizer_txt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}