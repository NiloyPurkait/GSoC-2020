{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "adverserial_trainingl.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiloyPurkait/GSoC-2020/blob/master/adverserial_trainingl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "## Adverserial training script\n",
        "\n",
        "Loads in pretrained discriminator and generator, and trains them in an adverserial fashion.\n",
        "\n",
        "- Adapted from : https://www.tensorflow.org/tutorials/generative/dcgan "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "from re import finditer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnrO2cH2hbJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pretraining import *\n",
        "from transformer_generator import *\n",
        "from transformer_discriminator import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XR8J2UYJYZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "file_path = \"/content/gdrive/My Drive/data/processed_graphs/eng/gat/f_data.txt\"\n",
        "test_path = \"/content/gdrive/My Drive/data/processed_graphs/eng/gat/test_data.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "\n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "DATA_MAX_LEN = 250\n",
        "target_vocab_size = tokenizer_txt.vocab_size + 2\n",
        "input_vocab_size = target_vocab_size\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1Ah3wWZDPuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                         epsilon=1e-9)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='train_accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UiysUa--4tOU",
        "colab": {}
      },
      "source": [
        "generator = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "generator.load_weights('./generator_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "colab": {}
      },
      "source": [
        "\n",
        "discriminator = TransformerDiscriminator(tokenizer_txt.vocab_size+2, maxlen=DATA_MAX_LEN)\n",
        "discriminator.load_weights('./discriminator_weights.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luctz2H04leO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def timer(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    t = time.time()\n",
        "    rv = func(*args, **kwargs)\n",
        "    print('Took ', time.time()-t, 'secs')\n",
        "    return rv\n",
        "  return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aeV4c2oRp9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def evaluate_batch(inp_batch):\n",
        "\n",
        "  output = tf.ones((tf.shape(inp_batch)[0], 1), tf.int32) * tokenizer_txt.vocab_size#inp_batch.numpy().shape\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        inp_batch, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    \n",
        "    predictions, attention_weights = generator(inp_batch, \n",
        "                                                 output,\n",
        "                                                 True,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return output#tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McoAxJNKXO5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_batch(preds, max_len):\n",
        "  disc_data = []\n",
        "  for sent in preds:\n",
        "    unparsed = decode_text(sent, tokenizer_txt)\n",
        "    retokenized = tokenizer_txt.encode(unparsed.split('<end>')[0]+'<end>')\n",
        "    padded = np.pad(np.array(retokenized), (0, max_len - len(retokenized)), 'constant')\n",
        "\n",
        "    disc_data.append(padded)\n",
        "\n",
        "  return np.array(disc_data)\n",
        "\n",
        "def pad_sequences(data, max_len):\n",
        "  stack = []\n",
        "  for i in data:\n",
        "    i = np.pad(np.array(i), (0, max_len - len(i)), 'constant')\n",
        "    if len(stack)==0:\n",
        "      stack = i\n",
        "    else:\n",
        "      stack = np.vstack((stack, i))\n",
        "  return stack\n",
        "\n",
        "\n",
        "def prepare_generated_data(rdf_batch, predicted_text_batch, max_len):\n",
        "  gen_data = np.concatenate((rdf_batch, predicted_text_batch), axis=1)\n",
        "  return pad_sequences(gen_data, max_len)\n",
        "\n",
        "def prepare_true_data(rdf_batch, text_batch, max_len):\n",
        "  \n",
        "  true_data = np.concatenate((rdf_batch, text_batch), axis=1)\n",
        "  return pad_sequences(true_data, max_len)\n",
        "\n",
        "def get_disc_batch(rdf_batch, txt_batch):\n",
        "  predicted_batch = evaluate_batch(rdf_batch)\n",
        "  predicted_batch = parse_batch(predicted_batch, GEN_DATA_MAX_LEN )\n",
        "  gen = prepare_generated_data(rdf_batch, predicted_batch, DATA_MAX_LEN)\n",
        "  true = prepare_true_data(rdf_batch, txt_batch, DATA_MAX_LEN)\n",
        "  return gen, true\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY-ZVeKOmjtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "GEN_DATA_MAX_LEN = 200#600\n",
        "DATA_MAX_LEN = 200#800"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cN5VeSlqnOs",
        "colab_type": "text"
      },
      "source": [
        "## Loss functions\n",
        "Seperate loss functions for discriminator and generator\n",
        "- Source : https://www.tensorflow.org/tutorials/generative/dcgan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITwhUYnKqciK",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator loss\n",
        "This method quantifies how well the discriminator is able to distinguish real sequences from fakes. It compares the discriminator's predictions on real sequences to an array of 1s, and the discriminator's predictions on fake (generated) sequences to an array of 0s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtRR-tJO6upk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    real_loss = loss_object(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = loss_object(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WjMZKrmqhT8",
        "colab_type": "text"
      },
      "source": [
        "### Generator loss\n",
        "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the generated sequenceq as real (or 1). Here, we will compare the discriminators decisions on the generated sequence to an array of 1s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF49xi3o6B3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Primary loss for plain adverserial training\n",
        "def generator_loss(fake_output):\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "  \n",
        "    return loss_object(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uAbKSVSS1NI5",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjd1O4Qc8D6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "#@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated, real = get_disc_batch(inp, tar)\n",
        "\n",
        "      real_output = discriminator(real, training=True)\n",
        "      fake_output = discriminator(generated, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    train_loss(gen_loss)\n",
        "    train_loss(disc_loss)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAwqls-DoF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset, tokenizer_txt = create_generator_dataset(file_path, BATCH_SIZE=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEhkdpKJHxRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=True\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWSRs65z5X6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if train:\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "\n",
        "      train_step(inp, tar)\n",
        "      \n",
        "      if batch % 1 == 0:\n",
        "\n",
        "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      \n",
        "      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                          ckpt_save_path))\n",
        "      \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                  train_loss.result(), \n",
        "                                                  train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}